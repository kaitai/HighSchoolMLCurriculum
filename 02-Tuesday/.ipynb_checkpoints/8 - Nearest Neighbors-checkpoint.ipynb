{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zxolp1hBrizd"
   },
   "source": [
    "# Using the $k$-Nearest Neighbor Algorithm\n",
    "\n",
    "In this notebook, we'll look at how we can use existing Python libraries to implement the $k$-nearest neighbor algorithm for the iris dataset. \n",
    "\n",
    "## Setting Things Up\n",
    "\n",
    "First, we need to import the libraries that we'll need.\n",
    "\n",
    "Pandas is a library for high-performance and easy to use data structures. Using this library makes machine learning both simpler and faster. Pandas give you many built-in tools to work with datasets and it handles dates and times very well!\n",
    "\n",
    "Numpy is a common and fundamental package for scientific computing. Many other libraries make use of numpy. The name comes from the words \"numerical\" and \"Python\" mashed together.\n",
    "\n",
    "Scikit-learn is a library for machine learning in Python. It does the work of implementing machine learning algorithms for us, and also contains commonly used datasets. Although we'll be using other things from Scikit-learn later, to start we only need to load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNCsioz1r4Rf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXD8l_ChscK4"
   },
   "source": [
    "Next, we'll load the data from the iris data set. In order to more easily work with this data, we'll store it as a pandas dataframe. The second line gathers the data from the iris dataset, and converts it into a pandas dataframe. This dataframe, iris_df, is what we'll be working with from now on. You can find more information about the iris dataset [here](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_l5LplCNuted"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris_df = pd.DataFrame(data = np.c_[iris['data'], iris['target']], columns = iris['feature_names'] + ['Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7b0ZoRZLV9lm"
   },
   "source": [
    "Now that we've got out data set up, we're ready to start experimenting with it!\n",
    "\n",
    "## Initial Investigation\n",
    "\n",
    "Before we apply the $k$-nearest neighbor algorithm to our dataset, let's do some preliminary investigation. This will serve to verify that our data seems to be set up correctly, and also give us a rough idea of how the data is distributed.\n",
    "\n",
    "First, let's check out how much data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1559926537444,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "ceSTqD9b2Rfv",
    "outputId": "db539384-b220-449e-bb8f-6c93952cdb08"
   },
   "outputs": [],
   "source": [
    "iris_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0yMFJtnWtN0"
   },
   "source": [
    "We have 150 data points, and each point has five coordinates. Four of these are the features of the flower: sepal length, sepal width, petal length, and petal width. In this example, the columns of the data come labelled with these names, but in data you find elsewhere that may not be automatically true. \n",
    "\n",
    "The last coordinate is the species of the flower. Let's see what values the species of the flower can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1559926539714,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "e30HdwtAXcZ8",
    "outputId": "e4e6efec-f31c-461f-d86c-dd25a8aa380c"
   },
   "outputs": [],
   "source": [
    "print(iris_df['Species'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49e-bz1sXq4k"
   },
   "source": [
    "This should seem a little strange - these should be species, not numbers!\n",
    "\n",
    "This is actually okay. Here, instead of storing a string with the name of each species, we store an integer corresponding to the species. The integer 0 corresponds to Iris-setosa, 1 corresponds to Iris-versicolor, and 2 corresponds to Iris-virginica.\n",
    "\n",
    "Next, let's look at some summary statistics for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1559926543485,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "xDtI0soQ2wVd",
    "outputId": "75cf728c-b687-4ef5-d04c-cae10779f25c"
   },
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAp32I45XPGv"
   },
   "source": [
    "Looking at these statistics for the features is interesting and useful information. However, averaging the species number for the all of the data points doesn't really make sense, so we can just ignore that column.\n",
    "\n",
    "Next, let's do some plotting so we can see what our data looks like. In order to do this, we need to import a couple more libraries.\n",
    "\n",
    "Matplotlib is a plotting library that aims to make plotting in Python easy. Seaborn is a library for statistical data visualization, which is based on matplotlib and makes much prettier plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vGjZhLK33qTj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCoXUqUabOts"
   },
   "source": [
    "The pairplot function will take two variables at a time, and plot the values of the datapoints for those to variables.\n",
    "\n",
    "The first parameter that we give this function is our dataframe, iris_df.\n",
    "\n",
    "The second parameter tells it which variables we want to plot. By entering iris_df.columns[:-1], we take all of the variables of iris_df except for the last one, which is the species. (The negative one (-1) tells the columns function to drop the last column from the end.) This way, we plot all of the features against each other, but not against the species. \n",
    "\n",
    "The last parameter tells the function to color the data points according to the their \"Species\" values. This way, we can easily see which species each data point is across all of the plots, and we can see how they are clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4453,
     "status": "ok",
     "timestamp": 1559926553197,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "UcrHfQ5E4SCh",
    "outputId": "e6421aef-b4e8-45d3-e745-475fd61f34ac"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.pairplot(iris_df, vars=iris_df.columns[:-1], hue = \"Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShDqDOzAcRlL"
   },
   "source": [
    "From our plots, we can see that the species do seem to be grouped according to these measurements. It seems like using the $k$-nearest neighbor algorithm will work well for this dataset!\n",
    "\n",
    "# $k$-Nearest Neighbor\n",
    "\n",
    "Using our preliminary data analysis, we expect that we'll be able to use the $k$-nearest neighbor algorithm effectively for this dataset, so that's what we'll do next.\n",
    "\n",
    "Since we want to see how the features can be used to predict the species of each iris, we'll separate our data into the features and the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OVO1mYwZkcm"
   },
   "outputs": [],
   "source": [
    "X = iris_df[iris.feature_names]\n",
    "y = iris_df[\"Species\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NxxA0aFQdDTi"
   },
   "source": [
    "Next, we'll split our data into a training set and a testing set. Fortunately, we don't have to do this by hand; there's a scikit-learn function that can do it for us!\n",
    "\n",
    "For the first parameter of train_test_split, we use the features. For the second parameter, we use the species, which is what we're trying to predict.\n",
    "\n",
    "The third parameter tells the function how large to make the test set. By stating test_size=0.2, we're deciding that 20% of the data will be used for testing.\n",
    "\n",
    "The last parameter determines how the data set is split. Here, we specify a random_state, to make sure that we'll get the same split each time we run this code. If we change 0 to another number, we'd get a different split. If we left off this parameter, the dataset would have a different random split each time we ran the code! This is often desirable, but for now, when we're just getting started, we'd like more predictable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Z0F_WlEdB7Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHk6B8MggRT-"
   },
   "source": [
    "Next, we'll import some more functions from scikit-learn. We now import the $k$-nearest neighbor classifier, along with some additional functions which will help us evaluate how well our classifier is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIQPV-1p4oYe"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96EitnDLgrcy"
   },
   "source": [
    "Next we build our classifier. To start out, we'll use the $3$-nearest neighbors. Later, we'll experiment with different values for $k$. After we've chosen our classifier, we train the classifier using our training set. These two short lines of code do all of the work of the $k$-nearest neighbor algorithm, which is pretty incredible!\n",
    "\n",
    "The first line tells the computer, \"Build a classifier model using KNeighbors\", and the second line says, \"Fit the model on my training data with X_train as the input and y_train as the classification.\" The computer will then build the k-nearest neighbors classifier using all of this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1559926563948,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "qjIFovZvDkO2",
    "outputId": "5dff34e4-16ae-4f6b-c136-08f8ecb2c95b"
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPeKQWVyhR2p"
   },
   "source": [
    "Now that we've built and trained our classifier, we use it to predict the species for our testing sets. This line of code says, \"Use the classifier we built on our test data, X_test, and make a prediction y_pred for the species.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8xH4XRaDv6O"
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pm3w2Gd5hbow"
   },
   "source": [
    "We've used our classifier to predict the species values for our testing set, and we'd like to compare these to the known species values for the testing set. To easily see how many data points were classified correctly, we use a function a find the confusion matrix. The rows are the actual species and the columns are the predicted species: we want all the non-zero entries to be on the diagonal of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1559926568948,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "1aSJonrDD19c",
    "outputId": "ea265550-0286-4509-e91d-63c542eddaf9"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MnucZiQ4jfFv"
   },
   "source": [
    "We see that only one data point from the test set was classified incorrectly - that seems pretty good! One of the flowers that was actually in species 2 was predicted to be in species 3 (there is a one in the second row, third column). \n",
    "\n",
    "We can also compute the accuracy score, which tells us what proportion of data points were predicted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1559926573464,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "u3GKZQxTD7ec",
    "outputId": "e7cdc50f-1212-428d-d947-0715107ff1cd"
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THS5ZjpAj6jQ"
   },
   "source": [
    "An accuracy score of 1 is perfect, so .966... is good! This means that 96.66...% of the data points from the testing set were classified correctly.\n",
    "\n",
    "# Finding the Optimal Value for $k$\n",
    "\n",
    "It seems like the $3$-nearest neighbor algorithm performs well for classifying irises, but is there a value of $k$ that would work even better? We'll now experiment with different values of $k$, to try to figure out the best possible value.\n",
    "\n",
    "We'll test values $k=1,3,5,7,...,49$, so we take the range from $1$ to $50$, going up by $2$ each time. For each value of $k$, we'll compute a cross-validation score, which will tell us the proportion of data points which are classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rdRNXNcEK_Z"
   },
   "outputs": [],
   "source": [
    "k_list = list(range(1,50,2))\n",
    "cv_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9X-ikGMpnEah"
   },
   "source": [
    "Now for the actual testing! For each value of $k$, we construct a $k$-nearest neighbor classifier. Then, we train our classifier. There's some potential for problems here - what if we got really lucky or unlucky with our split between training and testing set, and wind up with results that don't accurately reflect the accuracy of our algorithm?\n",
    "\n",
    "Using cross-validation fixes this problem. Instead of training and testing once on the given split, the cross_val_score function creates 10 variations on the training and testing sets, and evaluates the accuracy of the classifier for each of these 10 splits. That way, we're automatically evaluating the $k$-nearest neighbor algorithm on 10 different training and testing splits, without having to perform these splits ourselves!\n",
    "\n",
    "Finally, we compute the average of the accuracies for all 10 trials, and record this as the overall score for the current value of $k$.\n",
    "\n",
    "At the end of this process, we have an array with an accuracy score for each value of $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jVpHXcgnEuU"
   },
   "outputs": [],
   "source": [
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = 'accuracy')\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QeGTxejN27TJ"
   },
   "source": [
    "We'll plot the scores that we found against the $k$ values, so we can see which value of $k$ gives the most accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1559926797783,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "erGNgY4YOFiQ",
    "outputId": "9bc8631d-92de-47a4-9970-7e3f3d454180"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Performance of K Nearest Neighbors Algorithm')\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.plot(k_list, cv_scores)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtNl7PaX4no0"
   },
   "source": [
    "From our graph, it looks like the $k$-nearest neighbor algorithm performs best for $k=9$ or $k=11$, for our dataset. We can verify this, by having Python check which value of $k$ gives the highest accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1559926597931,
     "user": {
      "displayName": "Melissa Lynn",
      "photoUrl": "",
      "userId": "05926261559474768829"
     },
     "user_tz": 300
    },
    "id": "7pGiouotO2US",
    "outputId": "ce1d5a18-17e7-4a22-a49d-4744bae0057d"
   },
   "outputs": [],
   "source": [
    "best_k = k_list[cv_scores.index(max(cv_scores))]\n",
    "print(best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfAPP8oA492f"
   },
   "source": [
    "We see that $k=9$ maximizes the accuracy score for the $k$-nearest neighbor algorithm for the iris dataset. Yay!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Nearest Neighbor.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
